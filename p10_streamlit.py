# -*- coding: utf-8 -*-
"""P10_notebookfin .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tEKJLAFqn-Yg97oO5Y_R0RRxPetqZKHe
"""

!pip freeze > requirements.txt

"""# PROJET P6

# **NOTEBOOK ANALYSE TEXTE
<img src="https://zupimages.net/up/21/41/h6mx.gif" width="200" height="200">
<div style="width:100%;text-align: center;">

IMPORT DES BIBLIOTHEQUES

code de l'ancie essai repris pour le nettoyer
"""

from google.colab import drive
import pandas as pd

drive.mount("/content/drive")
df = pd.read_csv("/content/drive/MyDrive/P6/flipkart.csv")



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import nltk
import string
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn import metrics, model_selection
from xgboost import XGBClassifier
import glob
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer

class TextAnalysis:
    def __init__(self, data_file):
        self.data_file = data_file
        nltk.download('wordnet')
        nltk.download('stopwords')
        nltk.download('punkt')

    def load_data(self):
        df = pd.read_csv(self.data_file)
        return df

    @staticmethod
    def niveau_cat(categorie, niveau):
        categorie = categorie.split('["')[1].split('"]')[0]
        cat = categorie.split(" >> ")
        if len(cat) < 3:
            cat = [cat[0], cat[1], "None"]
            if len(cat) < 2:
                cat = [cat[0], "None", "None"]
        return cat[niveau]

    def niveau_cat_1(self, categorie):
        return self.niveau_cat(categorie, 0)

    def niveau_cat_2(self, categorie):
        return self.niveau_cat(categorie, 1)

    def niveau_cat_3(self, categorie):
        return self.niveau_cat(categorie, 2)

    def detail_categories(self, dataframe):
        dataframe["branche_categorie_1"] = dataframe["product_category_tree"].apply(self.niveau_cat_1)
        dataframe["branche_categorie_2"] = dataframe["product_category_tree"].apply(self.niveau_cat_2)
        dataframe["branche_categorie_3"] = dataframe["product_category_tree"].apply(self.niveau_cat_3)
        return dataframe.drop(["product_category_tree"], axis=1)

    @staticmethod
    def tokenize_sentence(sentence):
        tokenizer = nltk.RegexpTokenizer(r"\w+")
        return tokenizer.tokenize(sentence)

    @staticmethod
    def stemm_sentence(sentence):
        ps = PorterStemmer()
        stemmed_words = []
        for w in TextAnalysis.tokenize_sentence(sentence):
            stemmed_words.append(ps.stem(w))
        return stemmed_words

    @staticmethod
    def lemmatize_sentence(sentence):
        lem = WordNetLemmatizer()
        stem = TextAnalysis.stemm_sentence(sentence)
        return [lem.lemmatize(word, "v") for word in stem]

    def preprocess_text(self, text):
        # Convert to lowercase
        text = text.lower()
        # Tokenization and lemmatization
        words = TextAnalysis.lemmatize_sentence(text)
        # Remove stop words
        stop_words = set(stopwords.words("english"))
        filtered_words = [w for w in words if w not in stop_words]
        return filtered_words

    def process_corpus(self, df):
        corpus = df['description'].tolist()
        preprocessed_corpus = [self.preprocess_text(text) for text in corpus]
        preprocessed_text = [" ".join(review) for review in preprocessed_corpus]
        vectorizer = TfidfVectorizer(max_features=50)
        X = vectorizer.fit_transform(preprocessed_text)
        return X

    def cluster_analysis(self, X, num_clusters):
        kmeans = KMeans(n_clusters=num_clusters, random_state=42)
        y_pred = kmeans.fit_predict(X)
        return y_pred

    def evaluate_clusters(self, y_true, y_pred):
        homogeneity = homogeneity_score(y_true, y_pred)
        completeness = completeness_score(y_true, y_pred)
        purity = self.purity_score(y_true, y_pred)
        silhouette = silhouette_score(X, y_pred)
        return homogeneity, completeness, purity, silhouette

    @staticmethod
    def purity_score(y_true, y_pred):
        contingency_matrix = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1), dtype=np.int64)
        for true_label, pred_label in zip(y_true, y_pred):
            contingency_matrix[true_label, pred_label] += 1
        purity = np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)
        return purity
    def perform_pca(self, data_pca, pinf):
        """
        Perform PCA on the input data and display the scree plot.

        Args:
            data_pca (pd.DataFrame): Input data for PCA.
            pinf1 (float): Percentage of data to retain.

        Returns:
            int: Number of components to retain.
        """



        n_comp = data_pca.shape[1]
        svd = TruncatedSVD(n_components=n_comp)
        svd.fit(X)

        explained_variance_ratio_cumsum = np.cumsum(svd.explained_variance_ratio_)
        n_components_needed = np.argmax(explained_variance_ratio_cumsum >= pinf / 100) + 1

        print("Number of components to retain", pinf, "% of the information:", n_components_needed)

        svd = TruncatedSVD(n_components=n_components_needed)
        svd.fit(X)

        # Eboulis des valeurs propres
        self.display_scree_plot(svd)

        return n_components_needed
        # Calculate the number of components needed to retain the specified percentage of data
        pca = PCA(svd_solver='full')
        pca.fit(data_pca)

        explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)
        n_components_needed = np.argmax(explained_variance_ratio_cumsum >= pinf1 / 100) + 1

        print("Number of components to retain", pinf1, "% of information:", n_components_needed)

        # Perform PCA with the desired number of components
        pca = PCA(n_components=n_components_needed)
        pca.fit(data_pca)

        # Display the scree plot
        plt.plot(range(1, n_comp + 1), pca.explained_variance_ratio_, marker='o')
        plt.xlabel("Component")
        plt.ylabel("Explained Variance Ratio")
        plt.title("Scree Plot")
        plt.show()

        return n_components_needed

    def run_analysis(self, num_clusters):
        df = self.load_data()
        data = self.detail_categories(df)
        X = self.process_corpus(data)
        label_encoder = LabelEncoder()
        y_true = label_encoder.fit_transform(data["branche_categorie_1"])
        y_pred = self.cluster_analysis(X, num_clusters)
        homogeneity, completeness, purity, silhouette = self.evaluate_clusters(y_true, y_pred)
        print("For n_clusters={}, the silhouette score is {}".format(num_clusters, silhouette))
        print("Homogénéité:", homogeneity)
        print("Complétude:", completeness)
        print("Pureté:", purity)
        print("Score de silhouette:", silhouette)


# Example usage
data_file = "/content/drive/MyDrive/P6/flipkart.csv"
analysis = TextAnalysis(data_file)
analysis.run_analysis(num_clusters=7)

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import nltk
import string
from sklearn.feature_extraction.text import CountVectorizer
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from sklearn import  metrics, model_selection
from xgboost import XGBClassifier
import glob
from sklearn.decomposition import PCA

from sklearn.metrics import silhouette_score, homogeneity_score, completeness_score
from sklearn.preprocessing import LabelEncoder

pip install matplotlib

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer

pip install pyparsing

"""INITIALISATION DE GOOGLE DRIVE

"""

from google.colab import drive

drive.mount("/content/drive")
df = pd.read_csv("/content/drive/MyDrive/P6/flipkart.csv")

"""Nettoyage rapide"""

df.isnull().sum()

"""#FONCTIONS"""

# Traitement des catégories
def niveau_cat(categorie, niveau):
    categorie = categorie.split('["')[1].split('"]')[0]
    cat = categorie.split(" >> ")
    if (len(cat)) < 3:
        cat = [cat[0], cat[1], "None"]
        if (len(cat)) < 2:
            cat = [cat[0], "None", "None"]
    return cat[niveau]


def niveau_cat_1(categorie):
    return niveau_cat(categorie, 0)


def niveau_cat_2(categorie):
    return niveau_cat(categorie, 1)


def niveau_cat_3(categorie):
    return niveau_cat(categorie, 2)


def detail_categories(dataframe):
    df["branche_categorie_1"] = df["product_category_tree"].apply(niveau_cat_1)
    df["branche_categorie_2"] = df["product_category_tree"].apply(niveau_cat_2)
    df["branche_categorie_3"] = df["product_category_tree"].apply(niveau_cat_3)
    return dataframe.drop(["product_category_tree"], axis=1)

"""#TRAITEMENT DES CATEGORIES"""

data = detail_categories(df)

data.groupby("branche_categorie_2").count()["uniq_id"].sort_values(ascending = False)

data.groupby('branche_categorie_3').count()['uniq_id'].sort_values(ascending = False)

print("branche_categorie_1 :", df["branche_categorie_1"].nunique())
print("branche_categorie_1 :", df["branche_categorie_2"].nunique())
print("branche_categorie_1 :", df["branche_categorie_3"].nunique())

"""7 catégories principales

62 secondaires

242 tertiaires

#TRAITEMENT DU CORPUS

transformation de la colonne en liste
"""

liste = df['description'].to_list()

liste[1]

#DEFINITION FONCTIONS TEXTES

def tokenize_sentence(sentence):
    tokenizer = nltk.RegexpTokenizer(r"\w+")
    return tokenizer.tokenize(sentence)

def stemm_sentence(sentence):
    ps = PorterStemmer()

    stemmed_words=[]
    for w in tokenize_sentence(sentence):
        stemmed_words.append(ps.stem(w))
    return stemmed_words

def lemmatize_sentence(sentence):
    lem = WordNetLemmatizer()
    stem = stemm_sentence(sentence)
    return [lem.lemmatize(word,"v") for word in stem]

"""Transformation en jetons"""

liste_res=[]

"""#traitement de chaque individu de la variable description"""

i=0
for bcl in liste:
    text=liste[i]
    i=i+1
    #mise en minuscules
    text.lower()
    #tokenisation etlemmatisation de la variable
    words=lemmatize_sentence(text)
    #supression des stops words
    stop_words=set(stopwords.words("english"))
    filtered_word=[]
    for w in words:
        if w not in stop_words:
            filtered_word.append(w)

    liste_res.append(filtered_word)

liste_res[0]

"""# Utilisation de TF-IDF pour la récurence des mots et transformation en vecteur avec TfidfVectorizer

# TF-IDF signifie Term Frequency- Inverse Term Frequency.

#Nous utilisont TF-IDF et non LDA
LDA est normalement utilisé pour l’apprentissage non supervisé, pas pour la classification. Il fournit un modèle génératif, pas un modèle discriminant ce qui le rend moins optimal pour la classification. LDA peut également être sensible au prétraitement des données et aux paramètres du modèle.
"""

#voulant utiliser TfidfVectorizer, et ne pouvant pas utiliser de list avec, une petite manipulation

liste_res=[" ".join(review) for review in liste_res]
print(liste_res)
vectorizer = TfidfVectorizer(max_features = 50)
X = vectorizer.fit_transform(liste_res)

print(TfidfVectorizer.decode)

print ((X.ndim))

"""Nous avons une matrice à deux dimensions"""

nb_clust=7

df['product_category_tree'].describe()

"""642 subcatégories différentes, inclassable nous allons essayer déjà de classer  le niveau 1"""

df.to_csv(r'/content/drive/MyDrive/P6/df_cat.csv')

from sklearn.cluster import KMeans

k = nb_clust
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
df["y"] = y_pred

def purity_score(y_true, y_pred):
    # Compute contingency matrix
    contingency_matrix = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1), dtype=np.int64)
    for true_label, pred_label in zip(y_true, y_pred):
        contingency_matrix[true_label, pred_label] += 1

    # Assign the most frequent label to each cluster
    cluster_labels = np.argmax(contingency_matrix, axis=0)

    # Calculate purity
    purity = np.sum(contingency_matrix[cluster_labels, np.arange(len(cluster_labels))]) / len(y_true)

    return purity

df["cluster_id1"] = kmeans.labels_

label_encoder = LabelEncoder()
y_true = label_encoder.fit_transform(df["branche_categorie_1"])

# Set the desired number of clusters
num_clusters = 7

# Initialize KMeans
n_init = 'auto'  # Set n_init to 'auto'
kmeans = KMeans(n_clusters=num_clusters, max_iter=50, n_init=n_init)
y_pred = kmeans.fit_predict(X_reduit)

# Homogénéité
homogeneity = homogeneity_score(y_true, y_pred)

# Complétude
completeness = completeness_score(y_true, y_pred)

def purity_score(y_true, y_pred):
    # Compute contingency matrix
    contingency_matrix = np.zeros((np.max(y_true) + 1, np.max(y_pred) + 1), dtype=np.int64)
    for true_label, pred_label in zip(y_true, y_pred):
        contingency_matrix[true_label, pred_label] += 1

    # Calculate purity
    purity = np.sum(np.max(contingency_matrix, axis=0)) / np.sum(contingency_matrix)
    return purity

# Pureté
purity = purity_score(y_true, y_pred)

# Score de silhouette
silhouette = silhouette_score(X, y_pred)

print("For n_clusters={}, the silhouette score is {}".format(num_clusters, silhouette))
print("Homogénéité:", homogeneity)
print("Complétude:", completeness)
print("Pureté:", purity)
print("Score de silhouette:", silhouette)

"""### NOUVEAU CODE"""

from sklearn.decomposition import TruncatedSVD

! pip install sentence_transformers

import torch
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
import pandas as pd
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from sklearn.metrics import homogeneity_score, completeness_score, silhouette_score
import numpy as np
import plotly.graph_objects as go
from sklearn.decomposition import PCA, TruncatedSVD

class CategoryClustering:
    def __init__(self, nb_comb = 50):
        self.model = SentenceTransformer('distilbert-base-nli-mean-tokens')
        self.nb_comb = nb_comb

    def display_scree_plot(self,pca):
        scree = pca.explained_variance_ratio_*100
        plt.bar(np.arange(len(scree))+1, scree)
        plt.plot(np.arange(len(scree))+1, scree.cumsum(),c="red",marker='o')
        plt.xlabel("rang de l'axe d'inertie")
        plt.ylabel("pourcentage d'inertie")
        plt.title("Eboulis des valeurs propres")
        plt.show(block=False)


    def perform_pca(self, data_pca, pinf):
        """
        Perform PCA on the input data and display the scree plot.

        Args:
            data_pca (pd.DataFrame): Input data for PCA.
            pinf1 (float): Percentage of data to retain.

        Returns:
            int: Number of components to retain.
        """



        n_comp = data_pca.shape[1]
        svd = TruncatedSVD(n_components=n_comp)
        svd.fit(X)

        explained_variance_ratio_cumsum = np.cumsum(svd.explained_variance_ratio_)
        n_components_needed = np.argmax(explained_variance_ratio_cumsum >= pinf / 100) + 1

        print("Number of components to retain", pinf, "% of the information:", n_components_needed)

        svd = TruncatedSVD(n_components=n_components_needed)
        svd.fit(X)

        # Eboulis des valeurs propres
        self.display_scree_plot(svd)

        return n_components_needed
        # Calculate the number of components needed to retain the specified percentage of data
        pca = PCA(svd_solver='full')
        pca.fit(data_pca)

        explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)
        n_components_needed = np.argmax(explained_variance_ratio_cumsum >= pinf1 / 100) + 1

        print("Number of components to retain", pinf1, "% of information:", n_components_needed)

        # Perform PCA with the desired number of components
        pca = PCA(n_components=n_components_needed)
        pca.fit(data_pca)

        # Display the scree plot
        plt.plot(range(1, n_comp + 1), pca.explained_variance_ratio_, marker='o')
        plt.xlabel("Component")
        plt.ylabel("Explained Variance Ratio")
        plt.title("Scree Plot")
        plt.show()

        return svd


    def purity_score(self, y_true, y_pred):
        """
        Calculate purity score for clustering evaluation.
        """
        contingency_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))), dtype=np.int64)
        unique_true_labels, true_label_counts = np.unique(y_true, return_counts=True)
        unique_pred_labels, pred_label_counts = np.unique(y_pred, return_counts=True)

        # Fill contingency matrix
        for i in range(len(y_true)):
            true_label_idx = np.where(unique_true_labels == y_true[i])[0][0]
            pred_label_idx = np.where(unique_pred_labels == y_pred[i])[0][0]
            contingency_matrix[true_label_idx, pred_label_idx] += 1

        purity = np.sum(np.amax(contingency_matrix, axis=0)) / len(y_true)
        return purity

    def niveau_cat(self, categorie, niveau):
        """
        Extract category level from the given category string.
        """
        categorie = categorie.split('["')[1].split('"]')[0]
        cat = categorie.split(" >> ")
        if len(cat) < 3:
            cat = [cat[0], cat[1], "None"]
        if len(cat) < 2:
            cat = [cat[0], "None", "None"]
        return cat[niveau]

    def niveau_cat_1(self, categorie):
        """
        Extract level 1 category from the given category string.
        """
        return self.niveau_cat(categorie, 0)

    def niveau_cat_2(self, categorie):
        """
        Extract level 2 category from the given category string.
        """
        return self.niveau_cat(categorie, 1)

    def niveau_cat_3(self, categorie):
        """
        Extract level 3 category from the given category string.
        """
        return self.niveau_cat(categorie, 2)

    def extract_keywords_bert(self, description):
        """
        Extract keywords using Sentence-BERT model from the given description.
        """
        keywords = []
        sentences = description.split('. ')
        if len(sentences) >= 3:
            sentence_embeddings = self.model.encode(sentences)
            num_sentences = len(sentences)
            top_3_indices = np.argsort(-sentence_embeddings.mean(axis=0))[:3]
            top_3_indices = [idx for idx in top_3_indices if idx < num_sentences]
            for index in top_3_indices:
                keyword = sentences[index]
                if keyword not in keywords:
                    keywords.append(keyword)
        return keywords

    from sklearn.decomposition import TruncatedSVD

    def perform_clustering(self, dataframe, nb_compo):
        """
        Perform category clustering on the given DataFrame.
        """
        dataframe = dataframe.sample(frac=1, random_state=42)
        dataframe["branche_categorie_1"] = dataframe["product_category_tree"].apply(self.niveau_cat_1)
        dataframe["branche_categorie_2"] = dataframe["product_category_tree"].apply(self.niveau_cat_2)
        dataframe["branche_categorie_3"] = dataframe["product_category_tree"].apply(self.niveau_cat_3)
        dataframe["sub_category"] = dataframe["product_category_tree"].apply(lambda x: x.split(' >> ')[-1])

        dataframe["keywords"] = ""
        with tqdm(total=len(dataframe)) as pbar:
            for i, row in dataframe.iterrows():
                keywords = self.extract_keywords_bert(row["description"])
                dataframe.at[i, "keywords"] = " ".join(keywords)
                pbar.update(1)

        dataframe["combined_features"] = dataframe["branche_categorie_1"] + " " + dataframe["branche_categorie_2"] + " " + dataframe["branche_categorie_3"] + " " + dataframe["keywords"]
        vectorizer = TfidfVectorizer()
        X = vectorizer.fit_transform(dataframe["combined_features"])
        num_clusters = 7
        print(f"nombre de clusters : {num_clusters}")
        svd = TruncatedSVD(n_components=50)
        X_reduced = svd.fit_transform(X)

        kmeans = KMeans(n_clusters=num_clusters)
        kmeans.fit(X_reduced)

        dataframe["cluster_label"] = kmeans.labels_



        # Update the dataframe with reduced vectors
        dataframe["reduced_vector"] = list(X_reduced)
                # Calcul des métriques
        homogeneity = homogeneity_score(dataframe["branche_categorie_1"], dataframe["cluster_label"])
        completeness = completeness_score(dataframe["branche_categorie_1"], dataframe["cluster_label"])
        purity = self.purity_score(dataframe["branche_categorie_1"], dataframe["cluster_label"])
        silhouette = silhouette_score(X, dataframe["cluster_label"])

        #Rendre les variables de l'instance accessibles
        self.homogeneity = homogeneity
        self.completeness = completeness
        self.purity = purity
        self.silhouette = silhouette
        self.dataframe = dataframe
        self.X_reduced = X_reduced

# Lecture du fichier CSV
df = pd.read_csv("/content/drive/MyDrive/P6/flipkart.csv")

# Définir le pourcentage d'échantillonnage (par exemple 0.5 = 50%)
pourcentage_echantillonnage = 1

# Effectuer l'échantillonnage aléatoire avec le pourcentage spécifié
if pourcentage_echantillonnage != 1:
    df = df.sample(frac=pourcentage_echantillonnage)


# ACP
pinf1 = 99.9  # Percentage of data to retain

# appel de la fonction ACP
n_components_needed = clustering.perform_pca(X, pinf1)

# Effectuer le clustering des catégories
clustering = CategoryClustering()
df_clustered = clustering.perform_clustering(df,50)

# Access the homogeneity score from the instance
homogeneity = clustering.homogeneity
completeness = clustering.completeness
purity = clustering.purity
X_reduced = clustering.X_reduced
silhouette = clustering.silhouette

print(f"homogeneity {homogeneity}")
print(f"completeness {completeness}")
print(f"purity {purity}")
print(f"silhouette {silhouette}")

"""50 composantes

homogeneity 0.8091655175759431

completeness 0.8535456889477943

purity 0.8466666666666667
"""

dataframe=clustering.dataframe
# TSNE
tsne = TSNE(n_components=2, random_state=42)
tsne_results = tsne.fit_transform(X_reduced)

plt.figure(figsize=(10, 8))
scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=dataframe["cluster_label"], cmap="viridis")
plt.legend(handles=scatter.legend_elements()[0], labels=range(num_clusters), title="Cluster")
plt.title("TSNE Visualization of Clusters")
plt.xlabel("TSNE Dimension 1")
plt.ylabel("TSNE Dimension 2")
plt.show()

# TSNE 3D
tsne = TSNE(n_components=3, random_state=42)
tsne_results = tsne.fit_transform(X_reduced)

fig = go.Figure(data=[go.Scatter3d(
    x=tsne_results[:, 0],
    y=tsne_results[:, 1],
    z=tsne_results[:, 2],
    mode='markers',
    marker=dict(
        size=5,
        color=dataframe["cluster_label"],
        colorscale='Viridis',
        opacity=0.8
    )
)])

fig.update_layout(
    title="TSNE Visualization of Clusters",
    scene=dict(
        xaxis=dict(title="TSNE Dimension 1"),
        yaxis=dict(title="TSNE Dimension 2"),
        zaxis=dict(title="TSNE Dimension 3")
    ),
    legend=dict(
        title="Cluster",
        itemsizing="constant"
    )
)

fig.show()

for i in range(num_clusters):
            cluster_df = dataframe[dataframe['cluster_label'] == i]
            categories_count = cluster_df['branche_categorie_1'].value_counts()
            categories_str = ""
            for category, count in categories_count.items():
                subcategories_count = cluster_df[cluster_df['branche_categorie_1'] == category]['branche_categorie_2'].nunique()
                subcategories_names = cluster_df[cluster_df['branche_categorie_1'] == category]['branche_categorie_2'].unique()
                categories_str += f"{category} ({subcategories_count} sous-catégories): {count}\n"
                for subcategory in subcategories_names:
                    categories_str += f" - {subcategory}\n"
            print(f"Cluster {i}:\n{categories_str}")

